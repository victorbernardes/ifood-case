{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba308251",
   "metadata": {},
   "source": [
    "# Notebook de ingestão Glue — Bronze e Silver\n",
    "\n",
    "Este notebook combina a lógica dos scripts `processamento_dados_bronze.py` e `processamento_dados_silver.py` para ser executado em um Glue Notebook (Glue Studio).\n",
    "\n",
    "O notebook faz:\n",
    "- Processamento Bronze: lê arquivos Parquet de um prefix S3, aplica casts explícitos conforme schema (yellow/green), adiciona `ano`, `mes` e `ingestion_ts` e grava em `s3://ifood-case-bronze/...` particionado por `ano`/`mes`.\n",
    "- Processamento Silver: lê as tabelas Bronze do catálogo (`dados_taxi_bronze.taxi_yellow` e `dados_taxi_bronze.taxi_green`), seleciona/renomeia colunas, adiciona `tipo_taxi` e grava em `s3://ifood-case-silver/corridas_taxi/` particionado por `ano`/`mes`.\n",
    "\n",
    "Instruções rápidas:\n",
    "- Configure o Glue Notebook para usar um role com permissões S3/Glue/Catalog/Athena.\n",
    "- Ajuste os caminhos `INPUT_*` / `OUTPUT_*` nas células abaixo se necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports e inicialização Glue/Spark\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import boto3\n",
    "import traceback\n",
    "\n",
    "# Em notebooks Glue a célula de inicialização pode já prover glueContext; garantimos criação segura\n",
    "try:\n",
    "    sc = SparkContext.getOrCreate()\n",
    "except Exception:\n",
    "    sc = SparkContext()\n",
    "glue_context = GlueContext(sc)\n",
    "spark = glue_context.spark_session\n",
    "job = None\n",
    "try:\n",
    "    args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "    job = Job(glue_context)\n",
    "    job.init(args['JOB_NAME'], args)\n",
    "except Exception:\n",
    "    # Em alguns ambientes de notebook getResolvedOptions falha; seguimos sem Job para testes locais\n",
    "    pass\n",
    "\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62740327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schemas (copiados dos scripts)\n",
    "yellow_schema = T.StructType([\n",
    "    T.StructField('VendorID', T.LongType(), True),\n",
    "    T.StructField('tpep_pickup_datetime', T.TimestampType(), True),\n",
    "    T.StructField('tpep_dropoff_datetime', T.TimestampType(), True),\n",
    "    T.StructField('passenger_count', T.LongType(), True),\n",
    "    T.StructField('trip_distance', T.DoubleType(), True),\n",
    "    T.StructField('RatecodeID', T.LongType(), True),\n",
    "    T.StructField('store_and_fwd_flag', T.StringType(), True),\n",
    "    T.StructField('PULocationID', T.LongType(), True),\n",
    "    T.StructField('DOLocationID', T.LongType(), True),\n",
    "    T.StructField('payment_type', T.LongType(), True),\n",
    "    T.StructField('fare_amount', T.DoubleType(), True),\n",
    "    T.StructField('extra', T.DoubleType(), True),\n",
    "    T.StructField('mta_tax', T.DoubleType(), True),\n",
    "    T.StructField('tip_amount', T.DoubleType(), True),\n",
    "    T.StructField('tolls_amount', T.DoubleType(), True),\n",
    "    T.StructField('improvement_surcharge', T.DoubleType(), True),\n",
    "    T.StructField('total_amount', T.DoubleType(), True),\n",
    "    T.StructField('congestion_surcharge', T.DoubleType(), True)\n",
    "])\n",
    "\n",
    "green_schema = T.StructType([\n",
    "    T.StructField('VendorID', T.LongType(), True),\n",
    "    T.StructField('lpep_pickup_datetime', T.TimestampType(), True),\n",
    "    T.StructField('lpep_dropoff_datetime', T.TimestampType(), True),\n",
    "    T.StructField('store_and_fwd_flag', T.StringType(), True),\n",
    "    T.StructField('RatecodeID', T.LongType(), True),\n",
    "    T.StructField('PULocationID', T.LongType(), True),\n",
    "    T.StructField('DOLocationID', T.LongType(), True),\n",
    "    T.StructField('passenger_count', T.LongType(), True),\n",
    "    T.StructField('trip_distance', T.DoubleType(), True),\n",
    "    T.StructField('fare_amount', T.DoubleType(), True),\n",
    "    T.StructField('extra', T.DoubleType(), True),\n",
    "    T.StructField('mta_tax', T.DoubleType(), True),\n",
    "    T.StructField('tip_amount', T.DoubleType(), True),\n",
    "    T.StructField('tolls_amount', T.DoubleType(), True),\n",
    "    T.StructField('improvement_surcharge', T.DoubleType(), True),\n",
    "    T.StructField('total_amount', T.DoubleType(), True),\n",
    "    T.StructField('payment_type', T.LongType(), True),\n",
    "    T.StructField('congestion_surcharge', T.DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56572a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers: casting, listing S3, e process_dataset (adaptado do seu script)\n",
    "def cast_columns(df, table_name):\n",
    "    if 'taxi_yellow' in table_name:\n",
    "        datetime_cols = ['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "        long_cols = ['VendorID', 'passenger_count', 'PULocationID', 'DOLocationID']\n",
    "        double_cols = ['trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount','RatecodeID', 'payment_type', 'trip_type',\n",
    "                       'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge']\n",
    "        string_cols = ['store_and_fwd_flag']\n",
    "    else:  # taxi_green\n",
    "        datetime_cols = ['lpep_pickup_datetime', 'lpep_dropoff_datetime']\n",
    "        long_cols = ['VendorID',  'PULocationID', 'DOLocationID', 'passenger_count']\n",
    "        double_cols = ['trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount','RatecodeID', 'payment_type', 'trip_type',\n",
    "                       'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge']\n",
    "        string_cols = ['store_and_fwd_flag', 'ehail_fee']\n",
    "\n",
    "    for col_name in datetime_cols:\n",
    "        df = df.withColumn(col_name, F.to_timestamp(F.col(col_name)))\n",
    "    for col_name in long_cols:\n",
    "        df = df.withColumn(col_name, F.col(col_name).cast(T.LongType()))\n",
    "    for col_name in double_cols:\n",
    "        df = df.withColumn(col_name, F.col(col_name).cast(T.DoubleType()))\n",
    "    for col_name in string_cols:\n",
    "        df = df.withColumn(col_name, F.col(col_name).cast(T.StringType()))\n",
    "    return df\n",
    "\n",
    "def cast_df_to_structtype(df, struct):\n",
    "    # Aplica casts conforme StructType e garante colunas presentes na ordem do schema\n",
    "    for field in struct.fields:\n",
    "        if field.name in df.columns:\n",
    "            df = df.withColumn(field.name, F.col(field.name).cast(field.dataType))\n",
    "        else:\n",
    "            df = df.withColumn(field.name, F.lit(None).cast(field.dataType))\n",
    "    schema_cols = [f.name for f in struct.fields]\n",
    "    other_cols = [c for c in df.columns if c not in schema_cols]\n",
    "    return df.select(*(schema_cols + other_cols))\n",
    "\n",
    "def list_s3_files(bucket_path):\n",
    "    client = boto3.client('s3')\n",
    "    bucket = bucket_path.replace('s3://', '').split('/')[0]\n",
    "    prefix = '/'.join(bucket_path.replace('s3://', '').split('/')[1:])\n",
    "    paginator = client.get_paginator('list_objects_v2')\n",
    "    files = []\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get('Contents', []) or []:\n",
    "            if obj['Key'].endswith('.parquet'):\n",
    "                files.append(f's3://{bucket}/{obj['Key']}')\n",
    "    return files\n",
    "\n",
    "def process_dataset(input_path: str, output_path: str, table_name: str, schema):\n",
    "    print(f'Iniciando processamento do dataset: {table_name}')\n",
    "    try:\n",
    "        files = list_s3_files(input_path)\n",
    "        if not files:\n",
    "            print(f'Nenhum arquivo encontrado em {input_path}')\n",
    "            return\n",
    "\n",
    "        for file in files:\n",
    "            print(f'Processando arquivo: {file}')\n",
    "            df = spark.read.parquet(file)\n",
    "            df = cast_columns(df, table_name)\n",
    "            # extrai partições do nome do arquivo (formato YYYY-MM no nome do arquivo)\n",
    "            df = df.withColumn('source_file', F.lit(file))\n",
    "            df = df.withColumn('ano', F.regexp_extract(F.col('source_file'), r'({4})-({2})', 1).cast(T.IntegerType())) \\\n",
    "                   .withColumn('mes', F.regexp_extract(F.col('source_file'), r'({4})-({2})', 2).cast(T.IntegerType())) \\\n",
    "                   .withColumn('ingestion_ts', F.current_timestamp()) \\\n",
    "                   .drop('source_file')\n",
    "\n",
    "            # Cast explícito para o schema fornecido (evita incompatibilidades no catálogo)\n",
    "            try:\n",
    "                df = cast_df_to_structtype(df, schema)\n",
    "            except Exception as e:\n",
    "                print(f'Falha ao aplicar cast para o schema: {e}; seguindo sem cast explícito')\n",
    "\n",
    "            # Grava em append particionado por ano/mes (escolha append para evitar sobrescrever todo o prefix)\n",
    "            df.write.mode('append') \\\n",
    "                  .format('parquet') \\\n",
    "                  .partitionBy('ano', 'mes') \\\n",
    "                  .option('compression', 'snappy') \\\n",
    "                  .save(output_path)\n",
    "\n",
    "        # Atualiza partições no Glue Catalog\n",
    "        spark.sql(f'MSCK REPAIR TABLE {table_name}')\n",
    "        print(f'Processamento finalizado com sucesso para {table_name}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print('--------------------------------------------------')\n",
    "        print(f'Erro ao processar {table_name}')\n",
    "        print(f'Input: {input_path}')\n",
    "        print(f'Destino: {output_path}')\n",
    "        print('Mensagem:', str(e))\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a9401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações (ajuste conforme ambiente)\n",
    "DATASETS = [\n",
    "    {\n",
    "        'input': 's3://ifood-case-repo/yellow/',\n",
    "        'table': 'dados_taxi_bronze.taxi_yellow',\n",
    "        'output': 's3://ifood-case-bronze/taxi_yellow/',\n",
    "        'schema': yellow_schema\n",
    "    },\n",
    "    {\n",
    "        'input': 's3://ifood-case-repo/green/',\n",
    "        'table': 'dados_taxi_bronze.taxi_green',\n",
    "        'output': 's3://ifood-case-bronze/taxi_green/',\n",
    "        'schema': green_schema\n",
    "    }\n",
    "]\n",
    "\n",
    "# Executa o processamento Bronze para cada dataset\n",
    "for ds in DATASETS:\n",
    "    process_dataset(ds['input'], ds['output'], ds['table'], ds['schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Passo Silver: ler Bronze do catálogo e escrever Silver particionado ---\n",
    "def process_bronze_table_to_silver(input_table: str, taxi_type: str):\n",
    "    print(f'Processando tabela Bronze: {input_table}')\n",
    "    try:\n",
    "        df = spark.table(input_table)\n",
    "        if taxi_type == 'yellow':\n",
    "            df_silver = df.select(\n",
    "                F.col('VendorID').alias('id_fornecedor'),\n",
    "                F.col('passenger_count').cast(T.IntegerType()).alias('quantidade_passageiros'),\n",
    "                F.col('total_amount').alias('valor_total'),\n",
    "                F.col('tpep_pickup_datetime').alias('data_inicio_viagem'),\n",
    "                F.col('tpep_dropoff_datetime').alias('data_fim_viagem'),\n",
    "                F.lit('yellow').alias('tipo_taxi'),\n",
    "                F.col('ano'),\n",
    "                F.col('mes')\n",
    "            )\n",
    "        else:\n",
    "            df_silver = df.select(\n",
    "                F.col('VendorID').alias('id_fornecedor'),\n",
    "                F.col('passenger_count').cast(T.IntegerType()).alias('quantidade_passageiros'),\n",
    "                F.col('total_amount').alias('valor_total'),\n",
    "                F.col('lpep_pickup_datetime').alias('data_inicio_viagem'),\n",
    "                F.col('lpep_dropoff_datetime').alias('data_fim_viagem'),\n",
    "                F.lit('green').alias('tipo_taxi'),\n",
    "                F.col('ano'),\n",
    "                F.col('mes')\n",
    "            )\n",
    "        return df_silver\n",
    "    except Exception as e:\n",
    "        print('Erro ao processar', input_table, e)\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Concatena e grava Silver\n",
    "bronze_tables = [\n",
    "    {'table': 'dados_taxi_bronze.taxi_yellow', 'tipo': 'yellow'},\n",
    "    {'table': 'dados_taxi_bronze.taxi_green', 'tipo': 'green'}\n",
    "]\n",
    "df_silver_all = None\n",
    "for bt in bronze_tables:\n",
    "    df_silver = process_bronze_table_to_silver(bt['table'], bt['tipo'])\n",
    "    if df_silver is not None:\n",
    "        if df_silver_all is None:\n",
    "            df_silver_all = df_silver\n",
    "        else:\n",
    "            df_silver_all = df_silver_all.unionByName(df_silver)\n",
    "\n",
    "output_silver_path = 's3://ifood-case-silver/corridas_taxi/'\n",
    "if df_silver_all is not None:\n",
    "    df_silver_all.write.mode('overwrite') \\\n",
    "        .format('parquet') \\\n",
    "        .partitionBy('ano', 'mes') \\\n",
    "        .option('compression', 'snappy') \\\n",
    "        .save(output_silver_path)\n",
    "\n",
    "    # Atualiza partições no Glue Catalog (MSCK REPAIR)\n",
    "    spark.sql('MSCK REPAIR TABLE dados_taxi_silver.corridas_taxi')\n",
    "    print('Ingestão para Silver finalizada com sucesso.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ec173",
   "metadata": {},
   "source": [
    "## Notas finais e próximos passos\n",
    "\n",
    "- Se encontrar erros HIVE_BAD_DATA (incompatibilidade de tipos), apague a partição S3 afetada e reexecute a célula Bronze após garantir que os casts corretos são aplicados.\n",
    "- Para testes locais, você pode apontar `DATASETS` para prefixes/paths de um bucket de teste e rodar célula-a-célula.\n",
    "- Quando terminar, se criou um `Job` via getResolvedOptions, chame `job.commit()` em uma célula final (opcional)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
